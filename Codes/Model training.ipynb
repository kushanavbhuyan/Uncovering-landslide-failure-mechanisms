{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d427bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries needed to run the TDA based method package ##\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from shapely.geometry import MultiPoint\n",
    "import random\n",
    "import math\n",
    "import shapely.affinity\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial import ConvexHull\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "#import gdal\n",
    "from pyproj import Proj,transform\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import numpy as np\n",
    "import elevation\n",
    "from osgeo import gdal\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "import random \n",
    "from gtda.plotting import plot_diagram\n",
    "from gtda.homology import VietorisRipsPersistence,SparseRipsPersistence,EuclideanCechPersistence\n",
    "from gtda.diagrams import Amplitude,NumberOfPoints,PersistenceEntropy\n",
    "from gtda.diagrams import Filtering\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def read_shapefiles (path_filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    function to read the shapefile from the local file path of landslide inventory\n",
    "    \n",
    "  \n",
    "    Parameters:\n",
    "         :path_filename (str): path to local inventory shapefiles\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "         read shapefile from file path\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return gpd.read_file(path_filename)\n",
    "\n",
    "\n",
    "def min_max_inventory(poly_data,lon_res,lat_res):\n",
    "\n",
    "    \"\"\"\n",
    "    function to calculate the bounding box coordinates of complete landslide inventory\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "          :poly_data (str): landslide polygon data in an inventory\n",
    "          :lon_res (float): longitude resolution\n",
    "          :lat_res (float): latitude resolution\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "         bounding box coordinates of landslide inventory region\n",
    "    \n",
    "    \"\"\"\n",
    "    data_coord=[]\n",
    "    for l in range((np.shape(poly_data)[0])):\n",
    "        if poly_data['geometry'][l].geom_type=='Polygon':\n",
    "            poly_xy=np.asarray(poly_data['geometry'][l].exterior.coords)  ## (lon,lat)\n",
    "            min_landslide_lon,max_landslide_lon=np.min(poly_xy[:,0]),np.max(poly_xy[:,0])\n",
    "            min_landslide_lat,max_landslide_lat=np.min(poly_xy[:,1]),np.max(poly_xy[:,1])\n",
    "            data_coord.append([min_landslide_lon,max_landslide_lon,min_landslide_lat,max_landslide_lat])\n",
    "    data_coord=np.asarray(data_coord) \n",
    "    kk=20\n",
    "    \n",
    "    return (np.min(data_coord[:,0])-kk*lon_res, np.max(data_coord[:,1])+kk*lon_res,np.min(data_coord[:,2])+kk*lat_res,np.max(data_coord[:,3])-kk*lat_res)\n",
    "\n",
    "\n",
    "def latlon_to_eastnorth (lonlat_polydata):\n",
    "    \n",
    "    \"\"\" \n",
    "    function to convert the (longitude latitude) coordinates of polygons to (easting, northing) coordinates\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "          :lonllat_polydata (array_like): \n",
    "                             longitude and latitude coordinates data\n",
    "                      \n",
    "    Returns:\n",
    "            (array_like)\n",
    "            easting and northing coordinates of landslide polygon data when polygon data has longitude latitude coordinates \n",
    "\n",
    "    \n",
    "     \n",
    "    \"\"\"\n",
    "     \n",
    "    east_north_polydata=[]\n",
    "    for i in range(np.shape(lonlat_polydata)[0]):\n",
    "        u = utm.from_latlon(lonlat_polydata[i][1], lonlat_polydata[i][0])   ### (lat,lon) to (east,north)\n",
    "        east_north_polydata.append([u[0],u[1]])\n",
    "    east_north_polydata=np.asarray(east_north_polydata) \n",
    "    return  east_north_polydata \n",
    "\n",
    "def download_dem(poly_data,dem_location,inventory_name):\n",
    "\n",
    "    \"\"\"\n",
    "    function to download the DEM corresponding to inventory region\n",
    "\n",
    "    Parameters:\n",
    "         :poly_data (str) : landslide polygon data in an inventory\n",
    "         :dem_location (str): provide the path where user wants to download DEM\n",
    "         :inventory_name (str): inventory_name to save the dem file\n",
    "\n",
    "    Returns:\n",
    "        (str) downloaded DEM file location for input landslide inventory\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    longitude_min,longitude_max,latitude_min,latitude_max=min_max_inventory(poly_data,0.00,-0.00)\n",
    "\n",
    "    total_number_of_tiles=(longitude_max-longitude_min)*(latitude_max-latitude_min)\n",
    "    print('total number of tiles:',total_number_of_tiles)\n",
    "    print(\"** Number of tiles should be less than 100 or depend on user device RAM **\" )\n",
    "    print('** only the folder location in dem_location option **')\n",
    "\n",
    "    \n",
    "    #inventory_name=input('**only tif name should be given')\n",
    "    #inventory_name='inventory%s'%np.random.randint(0,1000)+'.tif'\n",
    "    final_output_filename=dem_location+inventory_name\n",
    "    if total_number_of_tiles<10:\n",
    "       longitude_min,longitude_max=longitude_min-0.4,longitude_max+0.4\n",
    "       latitude_min,latitude_max=latitude_min-0.4,latitude_max+0.4\n",
    "       latitude_min,latitude_max\n",
    "       print(\"less than 10 tiles\") \n",
    "       elevation.clip(bounds=(longitude_min, latitude_min, longitude_max, latitude_max), output=final_output_filename)\n",
    "       elevation.clean() \n",
    "\n",
    "    else:\n",
    "        print('more than 10 tiles')\n",
    "        latitude_width=latitude_max-latitude_min\n",
    "        longitude_width=longitude_max-longitude_min\n",
    "\n",
    "        add_latitude=3-latitude_width%3\n",
    "        add_longitude=3-longitude_width%3\n",
    "\n",
    "        latitude_max=latitude_max+add_latitude\n",
    "        longitude_max=longitude_max+add_longitude\n",
    "\n",
    "        latitude_width=(latitude_max-latitude_min)\n",
    "        longitude_width=(longitude_max-longitude_min)\n",
    "        t=0\n",
    "        for j in range(0,latitude_width,3):\n",
    "            for i in range(0,longitude_width,3):\n",
    "                t=t+1\n",
    "                output=dem_location+'inven_name%s.tif'%t\n",
    "                elevation.clip(bounds=(longitude_min+i, latitude_max-j-3, longitude_min+i+3,latitude_max-j), output=output)    \n",
    "                elevation.clean()\n",
    "\n",
    "        NN=10800\n",
    "        DEM_DATA=np.zeros((NN*latitude_width//3, NN*longitude_width//3),dtype='uint16')\n",
    "        t=1\n",
    "        X_0,Y_0=[],[]\n",
    "\n",
    "\n",
    "        for i in range(latitude_width//3):\n",
    "            for j in range(longitude_width//3):\n",
    "                inv_name=\"inven_name%s.tif\"%t\n",
    "                data_name=dem_location+inv_name\n",
    "                DEM = gdal.Open(data_name)\n",
    "                x_0,x_res,_,y_0,_,y_res = DEM.GetGeoTransform()\n",
    "                X_0.append(x_0),Y_0.append(y_0)\n",
    "                print(x_0,x_res,_,y_0,_,y_res)\n",
    "                #print(np.asarray(DEM))\n",
    "                from PIL import Image\n",
    "                #im = Image.open(data_name)\n",
    "                #z = np.array(DEM.GetRasterBand().ReadAsArray())\n",
    "\n",
    "                z=gdal.Dataset.ReadAsArray(DEM)\n",
    "                DEM_DATA[(i*NN):(i*NN)+NN,(j*NN):(j*NN)+NN]=z\n",
    "                t=t+1\n",
    "                print(t)\n",
    "        x_0=min(X_0)\n",
    "        y_0=max(Y_0)\n",
    "        time.sleep(180)\n",
    "        #######################################################################################################\n",
    "        geotransform = (x_0,x_res,0,y_0,0,y_res)\n",
    "        driver = gdal.GetDriverByName('Gtiff')\n",
    "        final_output_filename=dem_location+inventory_name\n",
    "        dataset = driver.Create(final_output_filename, DEM_DATA.shape[1], DEM_DATA.shape[0], 1, gdal.GDT_Float32)\n",
    "        dataset.SetGeoTransform(geotransform)\n",
    "        dataset.GetRasterBand(1).WriteArray(DEM_DATA)\n",
    "        #################################################################################################    \n",
    "    time.sleep(180)\n",
    "    return  final_output_filename \n",
    "\n",
    "\n",
    "def make_3d_polygons(poly_data,dem_location,inventory_name,kk):\n",
    "\n",
    "    \"\"\"    \n",
    "    function to get 3D point cloud from 2D shape of landslide\n",
    "\n",
    "    Parameters:\n",
    "       :poly_data (str): polygons shapefile\n",
    "       :dem_location (str): path of dem file\n",
    "       :inventory_name (str): path of dem file\n",
    "       :kk (int): kk=1 if user have already DEM corresponding to inventory region otherwise use any other number\n",
    "   \n",
    "    Returns:\n",
    "       (array_like) 3D data of landslides\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    if kk==1:  \n",
    "       DEM_FILE_NAME=dem_location+inventory_name\n",
    "    else:\n",
    "         DEM_FILE_NAME=download_dem(poly_data,dem_location,inventory_name)\n",
    "    ############################################################################\n",
    "    inProj = Proj(init='epsg:4326')\n",
    "    outProj = Proj(init='epsg:3857')\n",
    "    data=[]\n",
    "    eleva_polyon=[]\n",
    "\n",
    "    DEM = gdal.Open(DEM_FILE_NAME)\n",
    "    lon_init,lon_res,_,lat_init,_,lat_res = DEM.GetGeoTransform()\n",
    "    DEM_data=gdal.Dataset.ReadAsArray(DEM)\n",
    "    #print(np.shape(DEM_data))\n",
    "\n",
    "    lon_all=np.arange(lon_init,lon_init+np.shape(DEM_data)[1]*lon_res,lon_res)\n",
    "    lat_all=np.arange(lat_init,lat_init+np.shape(DEM_data)[0]*lat_res,lat_res)\n",
    "    \n",
    "    #print (' ***  Upload Complete Shapefiles Of Landslides In Landslide Inventory ***')\n",
    "    #print('*** Input should be a shapefiles of landslide polygons  *** ' )\n",
    "    \n",
    "    inv_lon_min,inv_lon_max,inv_lat_min,inv_lat_max=min_max_inventory(poly_data,lon_res,lat_res)\n",
    "    indices_lon_dem_crop_inventory=np.argwhere((lon_all>inv_lon_min)&(lon_all<inv_lon_max))[:,0]\n",
    "    indices_lat_dem_crop_inventory=np.argwhere((lat_all>inv_lat_min)&(lat_all<inv_lat_max))[:,0]\n",
    "\n",
    "    min_indices_lon_dem_crop_inventory=np.min(indices_lon_dem_crop_inventory)\n",
    "    max_indices_lon_dem_crop_inventory=np.max(indices_lon_dem_crop_inventory)\n",
    "\n",
    "    min_indices_lat_dem_crop_inventory=np.min(indices_lat_dem_crop_inventory)\n",
    "    max_indices_lat_dem_crop_inventory=np.max(indices_lat_dem_crop_inventory)\n",
    "    \n",
    "    DEM_data=DEM_data[min_indices_lat_dem_crop_inventory:max_indices_lat_dem_crop_inventory,\n",
    "                          min_indices_lon_dem_crop_inventory:max_indices_lon_dem_crop_inventory]\n",
    "    \n",
    "    lon_all=lon_all[min_indices_lon_dem_crop_inventory:max_indices_lon_dem_crop_inventory]\n",
    "    lat_all=lat_all[min_indices_lat_dem_crop_inventory:max_indices_lat_dem_crop_inventory]          ### check \n",
    "    \n",
    "    for l in range((np.shape(poly_data)[0])):\n",
    "        print(l)\n",
    "    #for l in range(100):    \n",
    "        if poly_data['geometry'][l].geom_type=='Polygon':\n",
    "            #print(l)\n",
    "            poly_xy=np.asarray(poly_data['geometry'][l].exterior.coords)  ## (lon,lat)\n",
    "            ze_1=poly_xy\n",
    "            if np.nanmin(ze_1) < 100:\n",
    "                ze_1=latlon_to_eastnorth(ze_1)\n",
    "                area_polygon=Polygon(ze_1).area\n",
    "                \n",
    "            if area_polygon>1000:    \n",
    "                #print(area_polygon) \n",
    "          \n",
    "                min_landslide_lon,max_landslide_lon=np.min(poly_xy[:,0]),np.max(poly_xy[:,0])\n",
    "                min_landslide_lat,max_landslide_lat=np.min(poly_xy[:,1]),np.max(poly_xy[:,1])\n",
    "\n",
    "                extra=10\n",
    "                indices_lon_land=np.argwhere((lon_all>min_landslide_lon-extra*lon_res) & (lon_all<max_landslide_lon+extra*lon_res))[:,0]\n",
    "                indices_lat_land=np.argwhere((lat_all>min_landslide_lat+extra*lat_res) & (lat_all<max_landslide_lat-extra*lat_res))[:,0]\n",
    "\n",
    "                DEM_landslide_region_crop=DEM_data[np.min(indices_lat_land):np.max(indices_lat_land)+1,\n",
    "                                                  np.min(indices_lon_land):np.max(indices_lon_land)+1] ############## check \n",
    "\n",
    "                lon_landslides_region=lon_all[indices_lon_land]\n",
    "                lat_landslides_region=lat_all[indices_lat_land]\n",
    "\n",
    "                ######## for landslide region interpolation #######\n",
    "                lon_mesh,lat_mesh=np.meshgrid(lon_landslides_region,lat_landslides_region)\n",
    "                lon_mesh,lat_mesh=lon_mesh.flatten(),lat_mesh.flatten()\n",
    "                DEM_landslide_region_crop_=DEM_landslide_region_crop.flatten()\n",
    "\n",
    "\n",
    "                lon_mesh_east,lat_mesh_north = transform(inProj,outProj,lon_mesh,lat_mesh)\n",
    "\n",
    "                poly_xy[:,0],poly_xy[:,1] = transform(inProj,outProj,poly_xy[:,0],poly_xy[:,1])\n",
    "\n",
    "\n",
    "                lon_mesh_east=np.reshape(lon_mesh_east,(np.shape(lon_mesh_east)[0],1))\n",
    "                lat_mesh_north=np.reshape(lat_mesh_north,(np.shape(lat_mesh_north)[0],1))\n",
    "                lonlat_mesh_eastnorth=np.hstack((lon_mesh_east,lat_mesh_north))\n",
    "\n",
    "                xmin1,xmax1=np.min(poly_xy[:,0])-30,np.max(poly_xy[:,0])+30\n",
    "                ymin1,ymax1=np.min(poly_xy[:,1])-30,np.max(poly_xy[:,1])+30\n",
    "                k,total_grid=0,32\n",
    "                xnew =np.linspace(xmin1-k, xmax1+k,total_grid)\n",
    "                ynew =np.linspace(ymin1-k, ymax1+k,total_grid) \n",
    "\n",
    "                xneww,yneww=np.meshgrid(xnew,ynew)\n",
    "\n",
    "\n",
    "                eleva_inter=griddata(lonlat_mesh_eastnorth, DEM_landslide_region_crop_,(xneww,yneww),method='cubic')\n",
    "                eleva_poly=griddata(lonlat_mesh_eastnorth, DEM_landslide_region_crop_,(poly_xy[:,0],poly_xy[:,1]),method='cubic')\n",
    "                poly_norm=(eleva_poly-np.min(eleva_poly))\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                eleva_final=eleva_inter\n",
    "                #eleva_norm=(eleva_final-np.min(eleva_final))/(np.max(eleva_final)-np.min(eleva_final))\n",
    "                eleva_norm=eleva_final\n",
    "                #######################################################################################################################\n",
    "                polygon = Polygon(poly_xy)\n",
    "                XNEW,YNEW=np.meshgrid(xnew,ynew)\n",
    "                XNEW,YNEW=XNEW.flatten(),YNEW.flatten()\n",
    "                combine_data=np.zeros((total_grid*total_grid,3))\n",
    "                combine_data[:,0]=XNEW\n",
    "                combine_data[:,1]=YNEW\n",
    "\n",
    "                  #print('elevation')\n",
    "                ELEVA_NORM=eleva_norm.flatten()\n",
    "                combine_data[:,2]=ELEVA_NORM\n",
    "\n",
    "                ##################################################################################################\n",
    "                indices=[]\n",
    "                for i in range(np.shape(combine_data)[0]):\n",
    "                    point=Point(combine_data[i,0:2])\n",
    "                    if polygon.contains(point)==True:\n",
    "                       indices.append(i) \n",
    "\n",
    "                indices=np.asarray(indices)\n",
    "                if np.shape(indices)[0]>0:\n",
    "                    combine_data=combine_data[indices]\n",
    "                    #combine_data[:,0]=(combine_data[:,0]-np.min(combine_data[:,0]))/(np.max(combine_data[:,0])-np.min(combine_data[:,0]))\n",
    "                    #combine_data[:,1]=(combine_data[:,1]-np.min(combine_data[:,1]))/(np.max(combine_data[:,1])-np.min(combine_data[:,1]))\n",
    "                    #combine_data[:,2]=(combine_data[:,2]-np.min(combine_data[:,2]))/(np.max(combine_data[:,2])-np.min(combine_data[:,2]))\n",
    "                    \n",
    "                    combine_data[:,0]=(combine_data[:,0]-np.min(combine_data[:,0]))\n",
    "                    combine_data[:,1]=(combine_data[:,1]-np.min(combine_data[:,1]))\n",
    "                    combine_data[:,2]=(combine_data[:,2]-np.min(combine_data[:,2]))\n",
    "                    \n",
    "                    data.append(combine_data)\n",
    "                    #eleva_polyon.append([poly_xy[:,0]-np.min(combine_data[:,0]),poly_xy[:,1]-np.min(combine_data[:,1]),poly_norm])\n",
    "                    a1=(poly_xy[:,0]-np.min(poly_xy[:,0]))/(np.max(poly_xy[:,0])-np.min(poly_xy[:,0]))\n",
    "                    b1=(poly_xy[:,1]-np.min(poly_xy[:,1]))/(np.max(poly_xy[:,1])-np.min(poly_xy[:,1]))\n",
    "                    c1=(poly_norm-np.min(poly_norm))/(np.max(poly_norm)-np.min(poly_norm))\n",
    "                    a1=a1[:,np.newaxis]\n",
    "                    b1=b1[:,np.newaxis]\n",
    "                    c1=c1[:,np.newaxis]\n",
    "                    \n",
    "                    eleva_polyon.append(np.hstack((a1,b1,c1)))\n",
    "\n",
    "\n",
    "    return eleva_polyon\n",
    "\n",
    "\n",
    "def get_ml_features(data):\n",
    "\n",
    "    \"\"\"\n",
    "    function to get machine learning features from 3D point cloud data\n",
    "\n",
    "    Parameters:\n",
    "         :data (array_like): 3D point cloud data of landslides\n",
    "   \n",
    "    Returns:\n",
    "          Topological features corresponding to 3D point cloud data\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    homology_dimensions = [0, 1, 2]\n",
    "    from gtda.homology import VietorisRipsPersistence,SparseRipsPersistence,EuclideanCechPersistence\n",
    "    persistence = VietorisRipsPersistence(metric=\"euclidean\",homology_dimensions=homology_dimensions,n_jobs=6,collapse_edges=True)\n",
    "    data= persistence.fit_transform(data)\n",
    "    data=Filtering(homology_dimensions=[0,1,2],epsilon=0.03).fit_transform(data)\n",
    "    #data=Filtering(homology_dimensions=[0],epsilon=2).fit_transform(data)\n",
    "   \n",
    "    \n",
    "    def average_lifetime(pers_diagrams_one):\n",
    "        homology_dimensions = [0, 1, 2]\n",
    "        persistence_diagram =pers_diagrams_one\n",
    "        persistence_table = pd.DataFrame(persistence_diagram, columns=[\"birth\", \"death\", \"homology_dim\"])\n",
    "        persistence_table[\"lifetime\"] = persistence_table[\"death\"] - persistence_table[\"birth\"] \n",
    "        life_avg_all_dims=[]\n",
    "\n",
    "        for dims in homology_dimensions:\n",
    "            avg_lifetime_one=persistence_table[persistence_table['homology_dim']==dims]['lifetime'].mean()\n",
    "            life_avg_all_dims.append(avg_lifetime_one)\n",
    "        life_avg_all_dims=np.asarray(life_avg_all_dims)\n",
    "        life_avg_all_dims=life_avg_all_dims.flatten() \n",
    "        return life_avg_all_dims   \n",
    "\n",
    "    metrics=[\"bottleneck\", \"wasserstein\", \"landscape\",'heat','betti',\"persistence_image\",\"silhouette\"]\n",
    "    feature_all_data=[]\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        if (i%2000)==0:\n",
    "            print(i) \n",
    "        feature_total_one=[]\n",
    "        persistant_one = data[i][None, :, :]\n",
    "        \n",
    "        persistence_entropy = PersistenceEntropy()\n",
    "        feature_onemeasure_entrophy = persistence_entropy.fit_transform(persistant_one)\n",
    "        feature_total_one.append(feature_onemeasure_entrophy)\n",
    "\n",
    "        feature_onemeasure=NumberOfPoints().fit_transform(persistant_one)\n",
    "        feature_total_one.append(feature_onemeasure)\n",
    "\n",
    "        feature_onemeasure=average_lifetime(data[i])  \n",
    "        feature_onemeasure=feature_onemeasure.reshape(1,3)\n",
    "        feature_total_one.append(feature_onemeasure)\n",
    "\n",
    "        for metric in metrics:\n",
    "            feature_onemeasure=Amplitude(metric=metric).fit_transform(persistant_one)\n",
    "            feature_total_one.append(feature_onemeasure)\n",
    "\n",
    "        feature_total_one=np.asarray(feature_total_one)     \n",
    "        feature_total_one=feature_total_one.flatten()\n",
    "\n",
    "        feature_all_data.append(feature_total_one)\n",
    "    feature_all_data=np.asarray(feature_all_data)  \n",
    "    return feature_all_data    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb7a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Italy features generated without interior point clouds\n",
    "features_slide_landslide=np.load('Sample feature engineered data of Italy for testing/slide.npy')\n",
    "\n",
    "features_dflow_landslide = np.load('Sample feature engineered data of Italy for testing/dflow.npy')\n",
    "\n",
    "features_eflow_landslide = np.load('Sample feature engineered data of Italy for testing/eflow.npy')\n",
    "\n",
    "features_complex_landslide=np.load('Sample feature engineered data of Italy for testing/complex.npy')\n",
    "\n",
    "features_fall_landslide=np.load('Sample feature engineered data of Italy for testing/fall.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a8be294",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_slide=np.zeros((np.shape(features_slide_landslide)[0],1))\n",
    "label_dflow=np.ones((np.shape(features_dflow_landslide)[0],1))\n",
    "label_eflow=np.ones((np.shape(features_eflow_landslide)[0],1))*2\n",
    "label_complex=np.ones((np.shape(features_complex_landslide)[0],1))*3\n",
    "label_fall=np.ones((np.shape(features_fall_landslide)[0],1))*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1891cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=2000 # Choose the number of samples you want to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.vstack((features_slide_landslide[0:N],features_dflow_landslide[0:N],features_eflow_landslide[0:N],features_complex_landslide[0:N],features_fall_landslide[0:N]))\n",
    "label=np.vstack((label_slide[0:N],label_dflow[0:N],label_eflow[0:N],label_complex[0:N],label_fall[0:N]))\n",
    "print(np.shape(data),np.shape(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08705dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_data_ind=np.unique(np.argwhere(data>10000)[:,0])\n",
    "len(false_data_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_ind=np.setdiff1d(np.arange(len(data)),false_data_ind)\n",
    "len(real_data_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4372c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[real_data_ind,:]\n",
    "label=label[real_data_ind,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "feature=[]\n",
    "for ee in range(10): \n",
    "    start_time = time.time()\n",
    "    print(ee)\n",
    "    Train,Test=[],[]\n",
    "    Trainlabel,Testlabel=[],[]\n",
    "    feato=[]\n",
    "    kf = KFold(n_splits=10,shuffle=True)  \n",
    "    for train_index, test_index in kf.split(data):\n",
    "        Train.append(data[train_index,:])\n",
    "        Test.append(data[test_index,:])\n",
    "        Trainlabel.append(label[train_index])\n",
    "        Testlabel.append(label[test_index])\n",
    "\n",
    "    for k in range(10):\n",
    "        Classifier = RandomForestClassifier(n_estimators=100) \n",
    "        scaler = StandardScaler()\n",
    "        train_data = scaler.fit_transform( Train[k])\n",
    "        test_data = scaler.transform(Test[k])\n",
    "        Classifier.fit(train_data,np.ravel(Trainlabel[k] ))      \n",
    "        y_pred = Classifier.predict(test_data)\n",
    "        Featur_importance=Classifier.feature_importances_\n",
    "        feato.append(Featur_importance)\n",
    "    feature.append(np.average(np.asarray(feato),axis=0)) \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))    \n",
    "FEATURE_IMP=np.asarray(feature)\n",
    "FEATURE_IMP=np.average(FEATURE_IMP,axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d426fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "imp_features = np.array([ 7, 6, 12, 21, 22, 10]) # Most important features\n",
    "DATA=data[:,imp_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ba0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "feature=[]\n",
    "\n",
    "for ee in range(5): \n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(ee)\n",
    "    Train,Test=[],[]\n",
    "    Trainlabel,Testlabel=[],[]\n",
    "    feato=[]\n",
    "    kf = KFold(n_splits=10,shuffle=True)  \n",
    "    for train_index, test_index in kf.split(DATA):\n",
    "        Train.append(data[train_index,:])\n",
    "        Test.append(data[test_index,:])\n",
    "        Trainlabel.append(label[train_index])\n",
    "        Testlabel.append(label[test_index])\n",
    "\n",
    "    for k in range(10):\n",
    "        Classifier = RandomForestClassifier(n_estimators=100,max_depth=None) \n",
    "        scaler = StandardScaler()\n",
    "        train_data = scaler.fit_transform(Train[k])\n",
    "        test_data = scaler.transform(Test[k])\n",
    "        Classifier.fit(train_data,np.ravel(Trainlabel[k]))      \n",
    "        y_pred = Classifier.predict(test_data)\n",
    "        print(accuracy_score(Testlabel[k],y_pred ))\n",
    "\n",
    "        \n",
    "        Featur_importance=Classifier.feature_importances_\n",
    "        feato.append(Featur_importance)\n",
    "    feature.append(np.average(np.asarray(feato),axis=0)) \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))    \n",
    "FEATURE_IMP=np.asarray(feature)\n",
    "FEATURE_IMP=np.average(FEATURE_IMP,axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b242665",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=confusion_matrix(Testlabel[k],y_pred)\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "(aa[0,0]/np.sum(aa[0,:]))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "(aa[1,1]/np.sum(aa[1,:]))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(aa[2,2]/np.sum(aa[2,:]))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(aa[3,3]/np.sum(aa[3,:]))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(aa[4,4]/np.sum(aa[4,:]))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(Testlabel[k],y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(Testlabel[k], y_pred)\n",
    "f1 = f1_score(Testlabel[k], y_pred, average='macro')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\n",
    "                                                                    'Slide', 'Debris Flow', 'Earth Flow', 'Complex Landslides', \n",
    "                                                                    'Rock Falls'\n",
    "                                                                    ])\n",
    "disp.plot()\n",
    "plt.title(f'F1 Score: {f1:.2f}')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
