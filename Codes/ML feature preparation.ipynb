{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5e61438",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Preparation and Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c371f11",
   "metadata": {},
   "source": [
    "## 1. Load the libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa0e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries needed to run the TDA based method package ##\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from shapely.geometry import MultiPoint\n",
    "import random\n",
    "import math\n",
    "import shapely.affinity\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial import ConvexHull\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "#import gdal\n",
    "from pyproj import Proj,transform\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import numpy as np\n",
    "import elevation\n",
    "from osgeo import gdal\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "import random \n",
    "from gtda.plotting import plot_diagram\n",
    "from gtda.homology import VietorisRipsPersistence,SparseRipsPersistence,EuclideanCechPersistence\n",
    "from gtda.diagrams import Amplitude,NumberOfPoints,PersistenceEntropy\n",
    "from gtda.diagrams import Filtering\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def read_shapefiles (path_filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    function to read the shapefile from the local file path of landslide inventory\n",
    "    \n",
    "  \n",
    "    Parameters:\n",
    "         :path_filename (str): path to local inventory shapefiles\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "         read shapefile from file path\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return gpd.read_file(path_filename)\n",
    "\n",
    "\n",
    "def min_max_inventory(poly_data,lon_res,lat_res):\n",
    "\n",
    "    \"\"\"\n",
    "    function to calculate the bounding box coordinates of complete landslide inventory\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "          :poly_data (str): landslide polygon data in an inventory\n",
    "          :lon_res (float): longitude resolution\n",
    "          :lat_res (float): latitude resolution\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "         bounding box coordinates of landslide inventory region\n",
    "    \n",
    "    \"\"\"\n",
    "    data_coord=[]\n",
    "    for l in range((np.shape(poly_data)[0])):\n",
    "        if poly_data['geometry'][l].geom_type=='Polygon':\n",
    "            poly_xy=np.asarray(poly_data['geometry'][l].exterior.coords)  ## (lon,lat)\n",
    "            min_landslide_lon,max_landslide_lon=np.min(poly_xy[:,0]),np.max(poly_xy[:,0])\n",
    "            min_landslide_lat,max_landslide_lat=np.min(poly_xy[:,1]),np.max(poly_xy[:,1])\n",
    "            data_coord.append([min_landslide_lon,max_landslide_lon,min_landslide_lat,max_landslide_lat])\n",
    "    data_coord=np.asarray(data_coord) \n",
    "    kk=20\n",
    "    \n",
    "    return (np.min(data_coord[:,0])-kk*lon_res, np.max(data_coord[:,1])+kk*lon_res,np.min(data_coord[:,2])+kk*lat_res,np.max(data_coord[:,3])-kk*lat_res)\n",
    "\n",
    "\n",
    "def latlon_to_eastnorth (lonlat_polydata):\n",
    "    \n",
    "    \"\"\" \n",
    "    function to convert the (longitude latitude) coordinates of polygons to (easting, northing) coordinates\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "          :lonllat_polydata (array_like): \n",
    "                             longitude and latitude coordinates data\n",
    "                      \n",
    "    Returns:\n",
    "            (array_like)\n",
    "            easting and northing coordinates of landslide polygon data when polygon data has longitude latitude coordinates \n",
    "\n",
    "    \n",
    "     \n",
    "    \"\"\"\n",
    "     \n",
    "    east_north_polydata=[]\n",
    "    for i in range(np.shape(lonlat_polydata)[0]):\n",
    "        u = utm.from_latlon(lonlat_polydata[i][1], lonlat_polydata[i][0])   ### (lat,lon) to (east,north)\n",
    "        east_north_polydata.append([u[0],u[1]])\n",
    "    east_north_polydata=np.asarray(east_north_polydata) \n",
    "    return  east_north_polydata \n",
    "\n",
    "def download_dem(poly_data,dem_location,inventory_name):\n",
    "\n",
    "    \"\"\"\n",
    "    function to download the DEM corresponding to inventory region\n",
    "\n",
    "    Parameters:\n",
    "         :poly_data (str) : landslide polygon data in an inventory\n",
    "         :dem_location (str): provide the path where user wants to download DEM\n",
    "         :inventory_name (str): inventory_name to save the dem file\n",
    "\n",
    "    Returns:\n",
    "        (str) downloaded DEM file location for input landslide inventory\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    longitude_min,longitude_max,latitude_min,latitude_max=min_max_inventory(poly_data,0.00,-0.00)\n",
    "\n",
    "    total_number_of_tiles=(longitude_max-longitude_min)*(latitude_max-latitude_min)\n",
    "    print('total number of tiles:',total_number_of_tiles)\n",
    "    print(\"** Number of tiles should be less than 100 or depend on user device RAM **\" )\n",
    "    print('** only the folder location in dem_location option **')\n",
    "\n",
    "    \n",
    "    #inventory_name=input('**only tif name should be given')\n",
    "    #inventory_name='inventory%s'%np.random.randint(0,1000)+'.tif'\n",
    "    final_output_filename=dem_location+inventory_name\n",
    "    if total_number_of_tiles<10:\n",
    "       longitude_min,longitude_max=longitude_min-0.4,longitude_max+0.4\n",
    "       latitude_min,latitude_max=latitude_min-0.4,latitude_max+0.4\n",
    "       latitude_min,latitude_max\n",
    "       print(\"less than 10 tiles\") \n",
    "       elevation.clip(bounds=(longitude_min, latitude_min, longitude_max, latitude_max), output=final_output_filename)\n",
    "       elevation.clean() \n",
    "\n",
    "    else:\n",
    "        print('more than 10 tiles')\n",
    "        latitude_width=latitude_max-latitude_min\n",
    "        longitude_width=longitude_max-longitude_min\n",
    "\n",
    "        add_latitude=3-latitude_width%3\n",
    "        add_longitude=3-longitude_width%3\n",
    "\n",
    "        latitude_max=latitude_max+add_latitude\n",
    "        longitude_max=longitude_max+add_longitude\n",
    "\n",
    "        latitude_width=(latitude_max-latitude_min)\n",
    "        longitude_width=(longitude_max-longitude_min)\n",
    "        t=0\n",
    "        for j in range(0,latitude_width,3):\n",
    "            for i in range(0,longitude_width,3):\n",
    "                t=t+1\n",
    "                output=dem_location+'inven_name%s.tif'%t\n",
    "                elevation.clip(bounds=(longitude_min+i, latitude_max-j-3, longitude_min+i+3,latitude_max-j), output=output)    \n",
    "                elevation.clean()\n",
    "\n",
    "        NN=10800\n",
    "        DEM_DATA=np.zeros((NN*latitude_width//3, NN*longitude_width//3),dtype='uint16')\n",
    "        t=1\n",
    "        X_0,Y_0=[],[]\n",
    "\n",
    "\n",
    "        for i in range(latitude_width//3):\n",
    "            for j in range(longitude_width//3):\n",
    "                inv_name=\"inven_name%s.tif\"%t\n",
    "                data_name=dem_location+inv_name\n",
    "                DEM = gdal.Open(data_name)\n",
    "                x_0,x_res,_,y_0,_,y_res = DEM.GetGeoTransform()\n",
    "                X_0.append(x_0),Y_0.append(y_0)\n",
    "                print(x_0,x_res,_,y_0,_,y_res)\n",
    "                #print(np.asarray(DEM))\n",
    "                from PIL import Image\n",
    "                #im = Image.open(data_name)\n",
    "                #z = np.array(DEM.GetRasterBand().ReadAsArray())\n",
    "\n",
    "                z=gdal.Dataset.ReadAsArray(DEM)\n",
    "                DEM_DATA[(i*NN):(i*NN)+NN,(j*NN):(j*NN)+NN]=z\n",
    "                t=t+1\n",
    "                print(t)\n",
    "        x_0=min(X_0)\n",
    "        y_0=max(Y_0)\n",
    "        time.sleep(180)\n",
    "        #######################################################################################################\n",
    "        geotransform = (x_0,x_res,0,y_0,0,y_res)\n",
    "        driver = gdal.GetDriverByName('Gtiff')\n",
    "        final_output_filename=dem_location+inventory_name\n",
    "        dataset = driver.Create(final_output_filename, DEM_DATA.shape[1], DEM_DATA.shape[0], 1, gdal.GDT_Float32)\n",
    "        dataset.SetGeoTransform(geotransform)\n",
    "        dataset.GetRasterBand(1).WriteArray(DEM_DATA)\n",
    "        #################################################################################################    \n",
    "    time.sleep(180)\n",
    "    return  final_output_filename \n",
    "\n",
    "\n",
    "def make_3d_polygons(poly_data,dem_location,inventory_name,kk):\n",
    "\n",
    "    \"\"\"    \n",
    "    function to get 3D point cloud from 2D shape of landslide\n",
    "\n",
    "    Parameters:\n",
    "       :poly_data (str): polygons shapefile\n",
    "       :dem_location (str): path of dem file\n",
    "       :inventory_name (str): path of dem file\n",
    "       :kk (int): kk=1 if user have already DEM corresponding to inventory region otherwise use any other number\n",
    "   \n",
    "    Returns:\n",
    "       (array_like) 3D data of landslides\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    if kk==1:  \n",
    "       DEM_FILE_NAME=dem_location+inventory_name\n",
    "    else:\n",
    "         DEM_FILE_NAME=download_dem(poly_data,dem_location,inventory_name)\n",
    "    ############################################################################\n",
    "    inProj = Proj(init='epsg:4326')\n",
    "    outProj = Proj(init='epsg:3857')\n",
    "    data=[]\n",
    "    eleva_polyon=[]\n",
    "\n",
    "    DEM = gdal.Open(DEM_FILE_NAME)\n",
    "    lon_init,lon_res,_,lat_init,_,lat_res = DEM.GetGeoTransform()\n",
    "    DEM_data=gdal.Dataset.ReadAsArray(DEM)\n",
    "    #print(np.shape(DEM_data))\n",
    "\n",
    "    lon_all=np.arange(lon_init,lon_init+np.shape(DEM_data)[1]*lon_res,lon_res)\n",
    "    lat_all=np.arange(lat_init,lat_init+np.shape(DEM_data)[0]*lat_res,lat_res)\n",
    "    \n",
    "    #print (' ***  Upload Complete Shapefiles Of Landslides In Landslide Inventory ***')\n",
    "    #print('*** Input should be a shapefiles of landslide polygons  *** ' )\n",
    "    \n",
    "    inv_lon_min,inv_lon_max,inv_lat_min,inv_lat_max=min_max_inventory(poly_data,lon_res,lat_res)\n",
    "    indices_lon_dem_crop_inventory=np.argwhere((lon_all>inv_lon_min)&(lon_all<inv_lon_max))[:,0]\n",
    "    indices_lat_dem_crop_inventory=np.argwhere((lat_all>inv_lat_min)&(lat_all<inv_lat_max))[:,0]\n",
    "\n",
    "    min_indices_lon_dem_crop_inventory=np.min(indices_lon_dem_crop_inventory)\n",
    "    max_indices_lon_dem_crop_inventory=np.max(indices_lon_dem_crop_inventory)\n",
    "\n",
    "    min_indices_lat_dem_crop_inventory=np.min(indices_lat_dem_crop_inventory)\n",
    "    max_indices_lat_dem_crop_inventory=np.max(indices_lat_dem_crop_inventory)\n",
    "    \n",
    "    DEM_data=DEM_data[min_indices_lat_dem_crop_inventory:max_indices_lat_dem_crop_inventory,\n",
    "                          min_indices_lon_dem_crop_inventory:max_indices_lon_dem_crop_inventory]\n",
    "    \n",
    "    lon_all=lon_all[min_indices_lon_dem_crop_inventory:max_indices_lon_dem_crop_inventory]\n",
    "    lat_all=lat_all[min_indices_lat_dem_crop_inventory:max_indices_lat_dem_crop_inventory]          ### check \n",
    "    \n",
    "    for l in range((np.shape(poly_data)[0])):\n",
    "        #print(l)\n",
    "    #for l in range(100):    \n",
    "        if poly_data['geometry'][l].geom_type=='Polygon':\n",
    "            #print(l)\n",
    "            poly_xy=np.asarray(poly_data['geometry'][l].exterior.coords)  ## (lon,lat)\n",
    "            ze_1=poly_xy\n",
    "            if np.nanmin(ze_1) < 100:\n",
    "                ze_1=latlon_to_eastnorth(ze_1)\n",
    "                area_polygon=Polygon(ze_1).area\n",
    "                \n",
    "            if area_polygon>500:    \n",
    "                #print(area_polygon) \n",
    "          \n",
    "                min_landslide_lon,max_landslide_lon=np.min(poly_xy[:,0]),np.max(poly_xy[:,0])\n",
    "                min_landslide_lat,max_landslide_lat=np.min(poly_xy[:,1]),np.max(poly_xy[:,1])\n",
    "\n",
    "                extra=10\n",
    "                indices_lon_land=np.argwhere((lon_all>min_landslide_lon-extra*lon_res) & (lon_all<max_landslide_lon+extra*lon_res))[:,0]\n",
    "                indices_lat_land=np.argwhere((lat_all>min_landslide_lat+extra*lat_res) & (lat_all<max_landslide_lat-extra*lat_res))[:,0]\n",
    "\n",
    "                DEM_landslide_region_crop=DEM_data[np.min(indices_lat_land):np.max(indices_lat_land)+1,\n",
    "                                                  np.min(indices_lon_land):np.max(indices_lon_land)+1] ############## check \n",
    "\n",
    "                lon_landslides_region=lon_all[indices_lon_land]\n",
    "                lat_landslides_region=lat_all[indices_lat_land]\n",
    "\n",
    "                ######## for landslide region interpolation #######\n",
    "                lon_mesh,lat_mesh=np.meshgrid(lon_landslides_region,lat_landslides_region)\n",
    "                lon_mesh,lat_mesh=lon_mesh.flatten(),lat_mesh.flatten()\n",
    "                DEM_landslide_region_crop_=DEM_landslide_region_crop.flatten()\n",
    "\n",
    "\n",
    "                lon_mesh_east,lat_mesh_north = transform(inProj,outProj,lon_mesh,lat_mesh)\n",
    "\n",
    "                poly_xy[:,0],poly_xy[:,1] = transform(inProj,outProj,poly_xy[:,0],poly_xy[:,1])\n",
    "\n",
    "\n",
    "                lon_mesh_east=np.reshape(lon_mesh_east,(np.shape(lon_mesh_east)[0],1))\n",
    "                lat_mesh_north=np.reshape(lat_mesh_north,(np.shape(lat_mesh_north)[0],1))\n",
    "                lonlat_mesh_eastnorth=np.hstack((lon_mesh_east,lat_mesh_north))\n",
    "\n",
    "                xmin1,xmax1=np.min(poly_xy[:,0])-30,np.max(poly_xy[:,0])+30\n",
    "                ymin1,ymax1=np.min(poly_xy[:,1])-30,np.max(poly_xy[:,1])+30\n",
    "                k,total_grid=0,32\n",
    "                xnew =np.linspace(xmin1-k, xmax1+k,total_grid)\n",
    "                ynew =np.linspace(ymin1-k, ymax1+k,total_grid) \n",
    "\n",
    "                xneww,yneww=np.meshgrid(xnew,ynew)\n",
    "\n",
    "\n",
    "                eleva_inter=griddata(lonlat_mesh_eastnorth, DEM_landslide_region_crop_,(xneww,yneww),method='cubic')\n",
    "                eleva_poly=griddata(lonlat_mesh_eastnorth, DEM_landslide_region_crop_,(poly_xy[:,0],poly_xy[:,1]),method='cubic')\n",
    "                poly_norm=(eleva_poly-np.min(eleva_poly))\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                eleva_final=eleva_inter\n",
    "                #eleva_norm=(eleva_final-np.min(eleva_final))/(np.max(eleva_final)-np.min(eleva_final))\n",
    "                eleva_norm=eleva_final\n",
    "                #######################################################################################################################\n",
    "                polygon = Polygon(poly_xy)\n",
    "                XNEW,YNEW=np.meshgrid(xnew,ynew)\n",
    "                XNEW,YNEW=XNEW.flatten(),YNEW.flatten()\n",
    "                combine_data=np.zeros((total_grid*total_grid,3))\n",
    "                combine_data[:,0]=XNEW\n",
    "                combine_data[:,1]=YNEW\n",
    "\n",
    "                  #print('elevation')\n",
    "                ELEVA_NORM=eleva_norm.flatten()\n",
    "                combine_data[:,2]=ELEVA_NORM\n",
    "\n",
    "                ##################################################################################################\n",
    "                indices=[]\n",
    "                for i in range(np.shape(combine_data)[0]):\n",
    "                    point=Point(combine_data[i,0:2])\n",
    "                    if polygon.contains(point)==True:\n",
    "                       indices.append(i) \n",
    "\n",
    "                indices=np.asarray(indices)\n",
    "                if np.shape(indices)[0]>0:\n",
    "                    combine_data=combine_data[indices]\n",
    "                    #combine_data[:,0]=(combine_data[:,0]-np.min(combine_data[:,0]))/(np.max(combine_data[:,0])-np.min(combine_data[:,0]))\n",
    "                    #combine_data[:,1]=(combine_data[:,1]-np.min(combine_data[:,1]))/(np.max(combine_data[:,1])-np.min(combine_data[:,1]))\n",
    "                    #combine_data[:,2]=(combine_data[:,2]-np.min(combine_data[:,2]))/(np.max(combine_data[:,2])-np.min(combine_data[:,2]))\n",
    "                    \n",
    "                    combine_data[:,0]=(combine_data[:,0]-np.min(combine_data[:,0]))\n",
    "                    combine_data[:,1]=(combine_data[:,1]-np.min(combine_data[:,1]))\n",
    "                    combine_data[:,2]=(combine_data[:,2]-np.min(combine_data[:,2]))\n",
    "                    \n",
    "                    data.append(combine_data)\n",
    "                    #eleva_polyon.append([poly_xy[:,0]-np.min(combine_data[:,0]),poly_xy[:,1]-np.min(combine_data[:,1]),poly_norm])\n",
    "                    a1=(poly_xy[:,0]-np.min(poly_xy[:,0]))/(np.max(poly_xy[:,0])-np.min(poly_xy[:,0]))\n",
    "                    b1=(poly_xy[:,1]-np.min(poly_xy[:,1]))/(np.max(poly_xy[:,1])-np.min(poly_xy[:,1]))\n",
    "                    c1=(poly_norm-np.min(poly_norm))/(np.max(poly_norm)-np.min(poly_norm))\n",
    "                    a1=a1[:,np.newaxis]\n",
    "                    b1=b1[:,np.newaxis]\n",
    "                    c1=c1[:,np.newaxis]\n",
    "                    \n",
    "                    eleva_polyon.append(np.hstack((a1,b1,c1)))\n",
    "\n",
    "    return eleva_polyon\n",
    "\n",
    "\n",
    "def get_ml_features(data):\n",
    "\n",
    "    \"\"\"\n",
    "    function to get machine learning features from 3D point cloud data\n",
    "\n",
    "    Parameters:\n",
    "         :data (array_like): 3D point cloud data of landslides\n",
    "   \n",
    "    Returns:\n",
    "          Topological features corresponding to 3D point cloud data\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    nan_ind=[]\n",
    "    for i in range(len(data)):\n",
    "        a1=np.any(np.isnan(data[i]))\n",
    "        if a1==True:\n",
    "           print(i,\"Damn !! Your data seems to have some NaN valued data. Next time, please be prepared.\") \n",
    "           nan_ind.append(i)\n",
    "        a2=np.isfinite(data[i].all()) #and gets True        \n",
    "        if a2==False:\n",
    "           print(i,\"Dammmmmmm !! You also have data with infinity??? What are you doing to me???\") \n",
    "           nan_ind.append(i)\n",
    "\n",
    "    data=np.delete(data,nan_ind , axis=0)\n",
    "    data=np.array(data).tolist() \n",
    "    \n",
    "    print(i,\"Don't worry!!! I found and deleted all problematic NaN and/or infinity valued data..... You are WELCOME\") \n",
    "    \n",
    "    homology_dimensions = [0, 1, 2]\n",
    "    from gtda.homology import VietorisRipsPersistence,SparseRipsPersistence,EuclideanCechPersistence\n",
    "    persistence = VietorisRipsPersistence(metric=\"euclidean\",homology_dimensions=homology_dimensions,n_jobs=6,collapse_edges=True)\n",
    "    data= persistence.fit_transform(data)\n",
    "    data=Filtering(homology_dimensions=[0,1,2],epsilon=0.03).fit_transform(data)\n",
    "    #data=Filtering(homology_dimensions=[0],epsilon=2).fit_transform(data)\n",
    "   \n",
    "    \n",
    "    def average_lifetime(pers_diagrams_one):\n",
    "        homology_dimensions = [0, 1, 2]\n",
    "        persistence_diagram =pers_diagrams_one\n",
    "        persistence_table = pd.DataFrame(persistence_diagram, columns=[\"birth\", \"death\", \"homology_dim\"])\n",
    "        persistence_table[\"lifetime\"] = persistence_table[\"death\"] - persistence_table[\"birth\"] \n",
    "        life_avg_all_dims=[]\n",
    "\n",
    "        for dims in homology_dimensions:\n",
    "            avg_lifetime_one=persistence_table[persistence_table['homology_dim']==dims]['lifetime'].mean()\n",
    "            life_avg_all_dims.append(avg_lifetime_one)\n",
    "        life_avg_all_dims=np.asarray(life_avg_all_dims)\n",
    "        life_avg_all_dims=life_avg_all_dims.flatten() \n",
    "        return life_avg_all_dims   \n",
    "\n",
    "    metrics=[\"bottleneck\", \"wasserstein\", \"landscape\",'heat','betti',\"persistence_image\",\"silhouette\"]\n",
    "    feature_all_data=[]\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        if (i%2000)==0:\n",
    "            print(i) \n",
    "        feature_total_one=[]\n",
    "        persistant_one = data[i][None, :, :]\n",
    "        \n",
    "        persistence_entropy = PersistenceEntropy()\n",
    "        feature_onemeasure_entrophy = persistence_entropy.fit_transform(persistant_one)\n",
    "        feature_total_one.append(feature_onemeasure_entrophy)\n",
    "\n",
    "        feature_onemeasure=NumberOfPoints().fit_transform(persistant_one)\n",
    "        feature_total_one.append(feature_onemeasure)\n",
    "\n",
    "        feature_onemeasure=average_lifetime(data[i])  \n",
    "        feature_onemeasure=feature_onemeasure.reshape(1,3)\n",
    "        feature_total_one.append(feature_onemeasure)\n",
    "\n",
    "        for metric in metrics:\n",
    "            feature_onemeasure=Amplitude(metric=metric).fit_transform(persistant_one)\n",
    "            feature_total_one.append(feature_onemeasure)\n",
    "\n",
    "        feature_total_one=np.asarray(feature_total_one)     \n",
    "        feature_total_one=feature_total_one.flatten()\n",
    "\n",
    "        feature_all_data.append(feature_total_one)\n",
    "    feature_all_data=np.asarray(feature_all_data)  \n",
    "    return feature_all_data    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f49e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data must be in the geographic coordinate system, both shapefiles and DEMs\n",
    "\n",
    "path_italy=\"path_to_folder/italy_lat_lon.shp\"\n",
    "gpd_italy=read_shapefiles(path_italy)\n",
    "\n",
    "dem_location_italy=\"path_to_folder/DEM/\"\n",
    "inventory_name_list_italy='Italy_DEM_Lat_Lon.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780ed2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Slides 109140\n",
      "Debris flow 20722\n",
      "Earth flow 72112\n",
      "Rockfalls 13787\n",
      "Complex Landslides 52422\n",
      "Spread 65\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "slide_type = gpd_italy.loc[gpd_italy['Type']=='slide'] \n",
    "slide_type = slide_type.reset_index()\n",
    "\n",
    "dflow_type = gpd_italy.loc[gpd_italy['nome_tipo']=='Colamento rapido'] \n",
    "dflow_type = dflow_type.reset_index()\n",
    "\n",
    "eflow_type = gpd_italy.loc[gpd_italy['nome_tipo']=='Colamento lento']\n",
    "eflow_type = eflow_type.reset_index()\n",
    "\n",
    "fall_type = gpd_italy.loc[gpd_italy['Type']=='fall']\n",
    "fall_type = fall_type.reset_index()\n",
    "\n",
    "complex_type = gpd_italy.loc[gpd_italy['Type']=='complex']\n",
    "complex_type = complex_type.reset_index()\n",
    "\n",
    "spread_type = gpd_italy.loc[gpd_italy['Type']=='spread']\n",
    "spread_type = spread_type.reset_index()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Slides\",np.shape(slide_type)[0])\n",
    "print(\"Debris flow\",np.shape(dflow_type)[0])\n",
    "print(\"Earth flow\",np.shape(eflow_type)[0])\n",
    "print(\"Rockfalls\",np.shape(fall_type)[0])\n",
    "print(\"Complex Landslides\",np.shape(complex_type)[0])\n",
    "print(\"Spread\",np.shape(spread_type)[0])\n",
    "print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0a5a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19310 Alright alright !!! I deleted those good for nothing imbeciles..... You are WELCOME\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "71814 Alright alright !!! I deleted those good for nothing imbeciles..... You are WELCOME\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "20000\n",
      "22000\n",
      "24000\n",
      "26000\n",
      "28000\n",
      "30000\n",
      "32000\n",
      "34000\n",
      "36000\n",
      "38000\n",
      "40000\n",
      "42000\n",
      "44000\n",
      "46000\n",
      "48000\n",
      "50000\n",
      "52000\n",
      "54000\n",
      "56000\n",
      "58000\n",
      "60000\n",
      "62000\n",
      "64000\n",
      "66000\n",
      "68000\n",
      "70000\n",
      "TDA features engineered !!\n",
      "time: 13h 2min 1s (started: 2023-06-30 18:11:29 +05:30)\n"
     ]
    }
   ],
   "source": [
    "pc_slide=make_3d_polygons(slide_type,dem_location_italy,inventory_name_list_italy,1)\n",
    "features_slide=get_ml_features(pc_slide)\n",
    "np.save(f\"path_to_folder/slide.npy\",features_slide)\n",
    "\n",
    "pc_dflow=make_3d_polygons(dflow_type,dem_location_italy,inventory_name_list_italy,1)\n",
    "features_dflow=get_ml_features(pc_dflow)\n",
    "np.save(f\"path_to_folder/dflow.npy\", features_dflow)\n",
    "\n",
    "pc_eflow=make_3d_polygons(eflow_type,dem_location_italy,inventory_name_list_italy,1)\n",
    "features_eflow=get_ml_features(pc_eflow)\n",
    "np.save(f\"path_to_folder/eflow.npy\", features_eflow)\n",
    "\n",
    "pc_complex=make_3d_polygons(complex_type,dem_location_italy,inventory_name_list_italy,1)\n",
    "features_complex=get_ml_features(pc_complex)\n",
    "np.save(f\"path_to_folder/complex.npy\", features_complex)\n",
    "\n",
    "pc_fall=make_3d_polygons(fall_type,dem_location_italy,inventory_name_list_italy,1)\n",
    "features_fall=get_ml_features(pc_fall)\n",
    "np.save(f\"path_to_folder/fall.npy\", features_fall)\n",
    "\n",
    "print(\"Your TDA features are engineered and saved locally!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
